# Tool Call Benchmark

**Evaluates LLM agents’ ability to select, sequence, and invoke tools correctly.**

Live at Streamlit [here](https://sxm-abet.streamlit.app/Tool%20Call%20Benchmark)

---

## Purpose

This benchmark measures:

* **Tool selection accuracy (TSA)**
* **Argument hallucination rate (AHR)**
* **Trajectory precision (TP)** — similarity of model’s tool-call sequence to ground truth

It simulates an agentic workflow where an LLM answers queries by correctly invoking deterministic tools.

---

## Pipeline Summary

```
TCBDatasetLoader → LangGraphAgentBuilder → AsyncConcurrentAgentRunner → TCBTranslator → TCBEvaluator → DashboardEvaluationSaver
```

---

# 1. Dataset Loading

**Loaded with:** `ToolCallBenchmarkDatasetLoader`

The dataset is a JSON list where each entry contains:

* `"input"`: user text
* `"target"`: expected ordered list of tool calls

The loader:

1. Reads the dataset from `dataset.json`
2. Builds a `ListDataset` with:

   * `inputs[i] = input text`
   * `targets[i] = expected tool sequence, names, and args`

The dataset is now ready to be consumed by the agent.

---

# 2. Agent Construction

Built using:

```
core.agentbuilder.LangGraphAgentBuilder
```

This builder:

* Loads the model (`gemini-2.5-flash` or any configured LLM)
* Loads tools from `benchmark/tool_call/tools.py`
* Loads a system prompt from `system_prompt.txt`
* Wraps tools using LangChain’s tool APIs
* Builds a LangGraph agent capable of tool calls

---

# 3. Running the Agent

The benchmark uses:

```
AsyncConcurrentAgentRunner
```

Meaning:

* Multiple dataset entries are processed at once
* Retries are supported
* Timeout and worker counts are configurable

For each sample:

```
raw_output = agent(input_text)
translated_output = translator(raw_output)
dataset.set_output(key, translated_output)
```

---

# 4. Translating Outputs

Translator: `ToolCallBenchmarkTranslator`

Translation steps:

1. Call `LangGraphTranslator` to convert LangGraph messages → internal messages (`AIMessage`, `ToolCallMessage`, etc.).
2. Extract:

   * `tool_call_names`: sequence of tools invoked
   * `tool_call_args`: arguments used per tool call

Produces:

```
ToolCallBenchmarkAgentOutput(
    messages=[...],
    tool_call_names=[...],
    tool_call_args=[...]
)
```

---

# 5. Evaluating Results

Evaluator: `ToolCallBenchmarkEvaluator`

Metrics computed:

### 1. Tool Selection Accuracy (TSA)
This measures **Recall**: The percentage of expected tools that the agent successfully identified.

$$
\text{TSA} = \frac{| T_{agent} \cap T_{gold} |}{| T_{gold} |}
$$

* **Where:**
    * $T_{agent}$ is the set of unique tool names generated by the agent.
    * $T_{gold}$ is the set of unique tool names in the Gold Standard.
    * $| \cdot |$ denotes the count (cardinality) of the set.
    * $\cap$ denotes the intersection (tools present in both).

### 2. Argument Hallucination Rate (AHR)
This measures **Schema Violation**: The percentage of arguments that are incorrect or invented. This is calculated **only** for tools where the Agent's tool name matches the Gold Standard.

$$
\text{AHR} = \frac{ \sum_{t \in M} (A_{invalid}) }{ \sum_{t \in M} (A_{total}) }
$$

* **Where:**
    * $M$ is the set of matched tools (where `Agent_Tool_Name == Gold_Tool_Name`).
    * $A_{total}$ is the total count of arguments generated by the agent for tool $t$.
    * $A_{invalid}$ is the count of arguments for tool $t$ that are either:
        1.  **Hallucinated Keys:** Parameter names not present in the function definition.
        2.  **Incorrect Values:** Values that differ from the Gold Standard (or fail validation).

### 3. Trajectory Precision (TP)
This measures **Sequential Accuracy**: The edit distance required to transform the agent's path into the correct path. This uses the **Levenshtein Similarity** method we discussed.

$$
\text{TP} = 1 - \frac{ \text{lev}(P_{agent}, P_{gold}) }{ \max(|P_{agent}|, |P_{gold}|) }
$$

* **Where:**
    * $P_{agent}$ is the ordered list of tool names generated by the agent.
    * $P_{gold}$ is the ordered list of tool names in the Gold Standard.
    * $|P|$ denotes the length of the list (number of steps).
    * $\text{lev}(a, b)$ is the Levenshtein Distance (minimum number of insertions, deletions, or substitutions to change $a$ to $b$).
    * *Note: If the max length is 0 (both empty), the score is 1.0.*

Samples include:

* Input text
* Expected tool sequence
* Actual output

---

# 6. Dashboard Integration

Evaluation results are saved via:

```
DashboardEvaluationSaver
```

And appear under:

```
evaluations/Tool Call Benchmark_<run_id>.json
```

Ready for visualization in the dashboard.

---

# Summary

This benchmark tests the core competencies of agentic LLMs in realistic tool-use scenarios, including:

* Call selection
* Tool argument grounding
* Multi-step reasoning with external functions
